{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADA_Project-Data_preprocessing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HiynRt4b3Cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0faede4e-6be8-48a7-a0a5-eb3330f4dbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'holoclean'...\n",
            "remote: Enumerating objects: 2267, done.\u001b[K\n",
            "remote: Total 2267 (delta 0), reused 0 (delta 0), pack-reused 2267\u001b[K\n",
            "Receiving objects: 100% (2267/2267), 8.28 MiB | 17.05 MiB/s, done.\n",
            "Resolving deltas: 100% (1500/1500), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HoloClean/holoclean.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r /content/CleanML/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOMSafC3FJtD",
        "outputId": "dfa72d84-ffd6-4329-e363-9d8852c29d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn==0.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: matplotlib==3.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: numpy==1.15.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 3)) (1.15.4)\n",
            "Requirement already satisfied: pandas==0.23.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 4)) (0.23.4)\n",
            "Requirement already satisfied: scikit-learn==0.20.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 5)) (0.20.0)\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: xgboost==0.81 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 7)) (0.81)\n",
            "Requirement already satisfied: statsmodels==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: openpyxl==2.5.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 9)) (2.5.6)\n",
            "Requirement already satisfied: XlsxWriter==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/CleanML/requirements.txt (line 10)) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.0->-r /content/CleanML/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.0->-r /content/CleanML/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.0->-r /content/CleanML/requirements.txt (line 2)) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.0->-r /content/CleanML/requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas==0.23.4->-r /content/CleanML/requirements.txt (line 4)) (2022.1)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.9.0->-r /content/CleanML/requirements.txt (line 8)) (0.5.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl==2.5.6->-r /content/CleanML/requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl==2.5.6->-r /content/CleanML/requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.0->-r /content/CleanML/requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.0.0->-r /content/CleanML/requirements.txt (line 2)) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUUG02IdFdT2",
        "outputId": "d781b2b7-04f5-4eba-a229-1990ded54195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 29.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=4a59b05e893bf36e22b861634e4c28f9b21ef04f17cfcae3f6148cccdb86118b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNV-eB7T3p0y",
        "outputId": "6d1dd924-8d61-42dc-82f8-0f2de6112bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chu-data-lab/CleanML.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA5uOWemxUum",
        "outputId": "81f5ea18-1c43-49b0-9a33-0dc677d3c5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CleanML' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q-PNwDCMCPa",
        "outputId": "002e585f-2187-48bf-e7f6-747bd9649a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "CY8Me-7ZFuGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cleanlab/cleanlab.git"
      ],
      "metadata": {
        "id": "TyxlGTVKecv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c5d459-972d-473c-e123-f061fd36b999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cleanlab'...\n",
            "remote: Enumerating objects: 4017, done.\u001b[K\n",
            "remote: Total 4017 (delta 0), reused 0 (delta 0), pack-reused 4017\u001b[K\n",
            "Receiving objects: 100% (4017/4017), 1.12 MiB | 5.63 MiB/s, done.\n",
            "Resolving deltas: 100% (2725/2725), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import sys\n",
        "import utils"
      ],
      "metadata": {
        "id": "0pihiuMpD_3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Outlier-Detection: SD,IQR,IF"
      ],
      "metadata": {
        "id": "dLll_mICD3dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SD(x, nstd=3.0):\n",
        "    # Standard Deviaiton Method (Univariate)\n",
        "    mean, std = np.mean(x), np.std(x)\n",
        "    cut_off = std * nstd\n",
        "    lower, upper = mean - cut_off, mean + cut_off\n",
        "    return lambda y: (y > upper) | (y < lower)\n",
        "\n",
        "def IQR(x, k=1.5):\n",
        "    # Interquartile Range (Univariate)\n",
        "    q25, q75 = np.percentile(x, 25), np.percentile(x, 75)\n",
        "    iqr = q75 - q25\n",
        "    cut_off = iqr * k\n",
        "    lower, upper = q25 - cut_off, q75 + cut_off\n",
        "    return lambda y: (y > upper) | (y < lower)\n",
        "\n",
        "def IF(x, contamination=0.01):\n",
        "    # Isolation Forest (Univariate)\n",
        "    IF = IsolationForest(contamination=contamination)\n",
        "    IF.fit(x.reshape(-1, 1))\n",
        "    return lambda y: (IF.predict(y.reshape(-1, 1)) == -1)"
      ],
      "metadata": {
        "id": "Q7UEzYx0DeI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Outlier-Repair: Mean,Median,Mode"
      ],
      "metadata": {
        "id": "N0_0JQerE71J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "    def fit(self, dataset, df):\n",
        "        num_df = df.select_dtypes(include='number')\n",
        "        cat_df = df.select_dtypes(exclude='number')\n",
        "        X = num_df.values\n",
        "        m = X.shape[1]\n",
        "\n",
        "        self.detectors = []\n",
        "        for i in range(m):\n",
        "            x = X[:, i]\n",
        "            detector = self.detect_fn(x, **self.kwargs)\n",
        "            self.detectors.append(detector)\n",
        "\n",
        "        ind = self.detect(df)\n",
        "        df_copy = df.copy()\n",
        "        df_copy[ind] = np.nan\n",
        "        self.repairer.fit(dataset, df_copy)\n",
        "        self.is_fit = True\n",
        "\n",
        "    def detect(self, df):\n",
        "        num_df = df.select_dtypes(include='number')\n",
        "        cat_df = df.select_dtypes(exclude='number')\n",
        "        X = num_df.values\n",
        "        m = X.shape[1]\n",
        "\n",
        "        ind_num = np.zeros_like(num_df).astype('bool')\n",
        "        ind_cat = np.zeros_like(cat_df).astype('bool')\n",
        "        for i in range(m):\n",
        "            x = X[:, i]\n",
        "            detector = self.detectors[i]\n",
        "            is_outlier = detector(x)\n",
        "            ind_num[:, i] = is_outlier\n",
        "\n",
        "        ind_num = pd.DataFrame(ind_num, columns=num_df.columns)\n",
        "        ind_cat = pd.DataFrame(ind_cat, columns=cat_df.columns)\n",
        "        ind = pd.concat([ind_num, ind_cat], axis=1).reindex(columns=df.columns)\n",
        "        return ind\n",
        "\n",
        "    def repair(self, df, ind):\n",
        "        df_copy = df.copy()\n",
        "        df_copy[ind] = np.nan\n",
        "        df_clean, _ = self.repairer.clean_df(df_copy)\n",
        "        return df_clean\n",
        "\n",
        "    def clean_df(self, df, ignore=None):\n",
        "        if not self.is_fit:\n",
        "            print(\"Must fit before clean\")\n",
        "            sys.exit()\n",
        "        ind = self.detect(df)\n",
        "        if ignore is not None:\n",
        "            ind.loc[:, ignore] = False\n",
        "        df_clean = self.repair(df, ind)\n",
        "        return df_clean, ind\n",
        "\n",
        "    def clean(self, dirty_train, dirty_test):\n",
        "        clean_train, indicator_train = self.clean_df(dirty_train)\n",
        "        clean_test, indicator_test = self.clean_df(dirty_test)\n",
        "        return clean_train, indicator_train, clean_test, indicator_test"
      ],
      "metadata": {
        "id": "YmOK6Ym6D7oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Outlier-Repair: Holoclean"
      ],
      "metadata": {
        "id": "G_tTZWZZEzDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def fit(self, dataset, df):\n",
        "        clean_raw_path = utils.get_dir(dataset, 'raw', '/content/airbnb_constraints.txt')\n",
        "        index_train_path = utils.get_dir(dataset, 'raw', 'idx_train.csv')\n",
        "        index_test_path = utils.get_dir(dataset, 'raw', 'idx_test.csv')\n",
        "\n",
        "        index_train = pd.read_csv(index_train_path).values.reshape(-1)\n",
        "        index_test = pd.read_csv(index_test_path).values.reshape(-1)\n",
        "        clean_raw = pd.read_csv(clean_raw_path)\n",
        "\n",
        "        if 'missing_values' in dataset['error_types']:\n",
        "            dirty_train = pd.read_csv(utils.get_dir(dataset, 'raw', 'dirty_train.csv'))\n",
        "            dirty_test = pd.read_csv(utils.get_dir(dataset, 'raw', 'dirty_test.csv'))\n",
        "            raw = pd.read_csv(utils.get_dir(dataset, 'raw', 'raw.csv'))\n",
        "            raw_mv_rows = raw.isnull().values.any(axis=1)\n",
        "            train_mv_rows = dirty_train.isnull().values.any(axis=1)\n",
        "            test_mv_rows = dirty_test.isnull().values.any(axis=1)\n",
        "\n",
        "            old_index = np.arange(len(raw))[raw_mv_rows == False]\n",
        "            new_index = np.arange(len(raw) - sum(raw_mv_rows))\n",
        "            index_map = {}\n",
        "\n",
        "            for o, n in zip(old_index, new_index):\n",
        "                index_map[o] = n\n",
        "\n",
        "            index_train_no_mv = index_train[train_mv_rows == False]\n",
        "            index_test_no_mv = index_test[test_mv_rows == False]\n",
        "\n",
        "            index_train = [index_map[i] for i in index_train_no_mv]\n",
        "            index_test = [index_map[i] for i in index_test_no_mv]\n",
        "\n",
        "        self.clean_train = clean_raw.iloc[index_train, :]\n",
        "        self.clean_test = clean_raw.iloc[index_test, :]\n",
        "\n",
        "    def clean(self, dirty_train, dirty_test):\n",
        "        indicator_train = pd.DataFrame(dirty_train.values != self.clean_train.values, columns=dirty_train.columns)\n",
        "        indicator_test = pd.DataFrame(dirty_test.values != self.clean_test.values, columns=dirty_train.columns)\n",
        "\n",
        "        clean_train = self.clean_train\n",
        "        clean_test =self.clean_test\n",
        "        return clean_train, indicator_train, clean_test"
      ],
      "metadata": {
        "id": "7FE9n7hmETkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Missing values-Repair: Mean_Dummy,Median_Dummy,Mode_Dummy"
      ],
      "metadata": {
        "id": "ge7ijsilLMXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def detect(self, df):\n",
        "        return df.isnull()\n",
        "\n",
        "    def fit(self, dataset, df):\n",
        "        if self.method == 'impute':\n",
        "            num_method = self.kwargs['num']\n",
        "            cat_method = self.kwargs['cat']\n",
        "            num_df = df.select_dtypes(include='number')\n",
        "            cat_df = df.select_dtypes(exclude='number')\n",
        "            if num_method == \"mean\":\n",
        "                num_imp = num_df.mean()\n",
        "            if num_method == \"median\":\n",
        "                num_imp = num_df.median()\n",
        "            if num_method == \"mode\":\n",
        "                num_imp = num_df.mode().iloc[0]\n",
        "\n",
        "            if cat_method == \"mode\":\n",
        "                cat_imp = cat_df.mode().iloc[0]\n",
        "            if cat_method == \"dummy\":\n",
        "                cat_imp = ['missing'] * len(cat_df.columns)\n",
        "                cat_imp = pd.Series(cat_imp, index=cat_df.columns)\n",
        "            self.impute = pd.concat([num_imp, cat_imp], axis=0)\n",
        "        self.is_fit = True\n",
        "\n",
        "    def repair(self, df):\n",
        "        if self.method == 'delete':\n",
        "            df_clean = df.dropna()\n",
        "\n",
        "        if self.method == 'impute':\n",
        "            df_clean = df.fillna(value=self.impute)\n",
        "        return df_clean\n",
        "\n",
        "    def clean_df(self, df):\n",
        "        if not self.is_fit:\n",
        "            print('Must fit before clean.')\n",
        "            sys.exit()\n",
        "        mv_mat = self.detect(df)\n",
        "        df_clean = self.repair(df)\n",
        "        return df_clean, mv_mat\n",
        "\n",
        "    def clean(self, dirty_train, dirty_test):\n",
        "        clean_train, indicator_train = self.clean_df(dirty_train)\n",
        "        clean_test, indicator_test = self.clean_df(dirty_test)\n",
        "        return clean_train, indicator_train, clean_test, indicator_test"
      ],
      "metadata": {
        "id": "N3rM2dgdEblp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Missing Values-Repair: Holoclean"
      ],
      "metadata": {
        "id": "EtGeMZLiLwyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def detect(self, df):\n",
        "        return df.isnull()\n",
        "\n",
        "    def fit(self, dataset, df):\n",
        "        clean_raw_path = utils.get_dir(dataset, 'raw', '/content/titanic_constraints.txt')\n",
        "        clean_raw = pd.read_csv(clean_raw_path)\n",
        "\n",
        "        index_train_path = utils.get_dir(dataset, 'raw', 'idx_train.csv')\n",
        "        index_test_path = utils.get_dir(dataset, 'raw', 'idx_test.csv')\n",
        "        index_train = pd.read_csv(index_train_path).values.reshape(-1)\n",
        "        index_test = pd.read_csv(index_test_path).values.reshape(-1)\n",
        "\n",
        "        self.clean_train = clean_raw.iloc[index_train, :]\n",
        "        self.clean_test = clean_raw.iloc[index_test, :]\n",
        "\n",
        "    def clean(self, dirty_train, dirty_test):\n",
        "        indicator_train = self.detect(dirty_train)\n",
        "        indicator_test = self.detect(dirty_test)\n",
        "\n",
        "        clean_train = self.clean_train\n",
        "        clean_test =self.clean_test\n",
        "        return clean_train, indicator_train, clean_test, indicator_test"
      ],
      "metadata": {
        "id": "cxdniGQRExYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Duplicates-Detect&Repair"
      ],
      "metadata": {
        "id": "T8usMy3ANhLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def fit(self, dataset, df):\n",
        "        self.keys = dataset['key_columns']\n",
        "    \n",
        "    def detect(self, df, keys):\n",
        "        key_col = pd.DataFrame(df, columns=keys)\n",
        "        is_dup = key_col.duplicated(keep='first')\n",
        "        is_dup = pd.DataFrame(is_dup, columns=['is_dup'])\n",
        "        return is_dup\n",
        "\n",
        "    def repair(self, df, is_dup):\n",
        "        not_dup = (is_dup.values == False)\n",
        "        df_clean = df[not_dup]\n",
        "        return df_clean\n",
        "\n",
        "    def clean_df(self, df):\n",
        "        is_dup = self.detect(df, self.keys)\n",
        "        df_clean = self.repair(df, is_dup)\n",
        "        return df_clean, is_dup\n",
        "\n",
        "    def clean(self, dirty_train, dirty_test):\n",
        "        clean_train, indicator_train = self.clean_df(dirty_train)\n",
        "        clean_test, indicator_test = self.clean_df(dirty_test)\n",
        "        return clean_train, indicator_train, clean_test, indicator_test"
      ],
      "metadata": {
        "id": "uOl4-mhLMahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanedDf(dataset):\n",
        "    # create saving folder\n",
        "    major_save_dir = utils.makedirs([config.data_dir, dataset[\"data_dir\"] + \"_major\", 'raw'])\n",
        "    minor_save_dir = utils.makedirs([config.data_dir, dataset[\"data_dir\"] + \"_minor\", 'raw'])\n",
        "    uniform_save_dir = utils.makedirs([config.data_dir, dataset[\"data_dir\"] + \"_uniform\", 'raw'])\n",
        "\n",
        "    # load clean data\n",
        "    clean_path = utils.get_dir(dataset, 'raw', 'raw.csv')\n",
        "    clean = utils.load_df(dataset, clean_path)\n",
        "    clean = clean.dropna().reset_index(drop=True)\n",
        "\n",
        "    major_clean_path = os.path.join(major_save_dir, 'mv_clean_raw.csv')\n",
        "    minor_clean_path = os.path.join(minor_save_dir, 'out_clean_raw.csv')\n",
        "    uniform_clean_path = os.path.join(uniform_save_dir, 'inconsist_clean_raw.csv')\n",
        "    clean.to_csv(major_clean_path, index=False)\n",
        "    clean.to_csv(minor_clean_path, index=False)\n",
        "    clean.to_csv(uniform_clean_path, index=False)\n",
        "\n",
        "    label = dataset['label']\n",
        "\n",
        "    # uniform flip\n",
        "    uniform = uniform_class_noise(clean, label)\n",
        "    # pairwise flip\n",
        "    major, minor = pairwise_class_noise(clean, label)\n",
        "\n",
        "    major_raw_path = os.path.join(major_save_dir, 'raw.csv')\n",
        "    minor_raw_path = os.path.join(minor_save_dir, 'raw.csv')\n",
        "    uniform_raw_path = os.path.join(uniform_save_dir, 'raw.csv')"
      ],
      "metadata": {
        "id": "wjDNT5aGNpYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZGPq4YWZ1Jmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}